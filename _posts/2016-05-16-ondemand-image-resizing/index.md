---
layout: post
date: 2016-05-16 10:30:00 +09:00
permalink: /2016-05-16-ondemand-image-resizing
disqusUrl: http://engineering.vcnc.co.kr/2016/05/ondemand-image-resizing/

title: '서버 비용을 70%나 줄인 온디맨드 리사이징 이야기'
thumbnail:
  - color: ./thumbnail-color.png
    gray: ./thumbnail-gray.png
description: 비트윈에서는 사용자들이 올린 사진들을 빠르고 비용 효율적으로 처리하기 위해서 많은 노력을 하였습니다.
  이번 포스팅에서는 비트윈에서 더 나은 사진 처리를 위해서 어떠한 아키텍쳐를 구성하고 있는지를 공유하고자 합니다.

tags:
  [
    '비트윈 서버',
    'AWS',
    'S3',
    'Skia',
    'EC2',
    'Resizer',
    'WebP',
    'CloudFront',
    'Between',
    'Spot Instance',
    'Auto-Scaling',
    'CloudWatch',
  ]

authors:
  - name: 'Andrew Kim'
    facebook: https://www.facebook.com/ewmkkpe
    link: https://www.facebook.com/ewmkkpe
---

비트윈의 서버에는 사용자들이 올리는 수많은 사진이 저장되어 있습니다.
2016년 3월 기준으로 커플들이 데이트에서 찍은 사진, 각자의 프로필 사진, 채팅을 나누며 올린 재미있는 짤방까지 약 11억 장의 사진이 저장되어 있습니다.
비트윈에서는 이러한 사용자들의 소중한 추억을 잘 보관하고, 사용자들의 요청을 빠르고 비용 효율적으로 처리하기 위해서 많은 노력을 기울이고 있습니다.
이번 포스팅에서는 비트윈 개발팀이 사용자들의 사진 처리를 보다 효율적으로 하기 위해서 어떠한 노력을 하였는지 공유하고자 합니다.

## 기존의 아키텍쳐

비트윈 사용자가 채팅창이나 모멘츠 탭에서 사진을 업로드 할 경우, 해당 사진은 업로더 서버라고 불리는 전 세계 각지에 퍼져 있는 사진 업로드 전용 서버 중 가장 가까운 서버를 자동으로 찾아서 업로드 됩니다.
업로더 서버는 사진을 해당 AWS Region의 [S3] bucket에 적재하고, 미리 지정된 크기의 썸네일을 자동으로 생성하여 역시 S3에 저장합니다.
그리고 Tokyo Region에 있는 비트윈 메인 서버에 이 결과를 토큰 형태로 전송하여 DB에 그 정보를 저장하도록 합니다.
이러한 과정을 통해서 일반 HTTP request보다 훨씬 큰 용량을 가지고 있는 사용자의 사진이 최대한 적은 지연시간을 가지고 업로드되도록 합니다.

![기존 사진 전송 아키텍쳐][oldarchitecture]

<figcaption>사용자가 올린 사진은 원본이 S3에 저장됨과 동시에 미리 정해진 사이즈로 썸네일을 생성해서 저장된다.</figcaption>

하나의 사진이 대략 5장에서 6장의 서로 다른 크기의 썸네일로 리사이징이 되는데, 이는 클라이언트의 디스플레이 크기에 따라서 최적화된 이미지를 내려주기 위함이었습니다.
예를 들어서 아주 작은 썸네일이면 충분한 채팅 프로필 표시 화면을 그리기 위해서 사용자가 올린 3백만 픽셀이나 되는 원본 사진을 받아서 클라이언트가 리사이징 하는 것은 지연 시간뿐 아니라 과도한 데이터 사용이라는 측면에서 효율적이지 않기 때문에 작게 리사이징 해놓은 사진을 내려주는 것이 더 바람직합니다.

비트윈 사용자들의 넘치는 사랑(?)에 비트윈은 출시 후 5년 동안 약 11억 장, 썸네일을 모두 합치면 66억 장의 사진을 저장하게 되었습니다.
이 사진은 전부 AWS S3에 저장되어 있으며, 썸네일을 합친 총 용량은 2016년 3월 기준 무려 738TB였습니다.
이에 따라 사진을 저장하기 위한 S3 비용이 전체 인프라 운영 비용에서 상당 부분을 차지하게 되었습니다.

## 기존 아키텍쳐의 비효율성

비트윈 팀은 어느 날 위와 같은 기존의 사진 전송 아키텍쳐에 의문을 가지게 되었습니다.
비트윈 서비스가 다른 서비스와 가장 다른 특징 중의 하나는 커플 간의 데이터는 그 둘 사이에서만 공유된다는 점입니다.
일반적인 웹사이트 같은 경우, 하나의 게시물 혹은 이미지가 수천 수 만명의 유저에게 전달되지만 비트윈에서는 그렇지 않습니다.
즉, 개별 사진의 [Fan-out]이 작다는 점을 특징으로 가지고 있습니다.

그리고 클라이언트에서 [LRU]를 기반으로 한 파일 캐쉬를 사용하고 있는데, 이를 통해서 위에서 말씀드린 채팅창 프로필 사진 같은 경우 클라이언트에서 캐쉬될 가능성이 매우 커지게 됩니다.
그리고 CDN으로 사용하고 있는 AWS의 CloudFront에서도 약 30~40%의 추가적인 Cache hit을 얻을 수 있었습니다.
즉, **이미 Fan-out이 낮은 리소스가 높은 Cache hit rate를 가지는 사용패턴**을 가지고 있는 셈이 됩니다.

더군다나 사용자의 디바이스 사이즈에 따라서 미리 리사이징 해놓은 썸네일 중 일부는 아예 사용하지 않는 사용패턴이 나타나기도 합니다.
아이패드와 같은 큰 디스플레이를 가진 클라이언트를 쓰는 사용자와 아이폰4를 사용하는 사용자가 필요로 하는 썸네일의 크기는 다를 수밖에 없기 때문입니다.

아래의 그래프는 S3 접근 로그를 분석해서 파악한 특정 기간 내에 같은 해상도를 가지는 썸네일을 클라이언트가 한 번 이상 재요청 하는 비율을 나타내는 그래프입니다.
하루 내에 같은 해상도의 사진을 요청하는 경우는 10% 가 되지 않으며, 한 달 안에도 33% 정도에 불과한 것을 알 수 있습니다.

![S3 Cache hit rate][s3cachehitrate]

<figcaption>특정 기간 내에 S3에 저장된 썸네일이 다시 요청되는 비율</figcaption>

결국 비트윈 팀은 미리 여러 해상도의 썸네일을 준비해서 저장해 놓은 아키텍쳐보다는 사용자가 요청할 때 그 요청에 알맞게 리사이징된 썸네일을 새로 생성해서 내려주는 게 훨씬 비용 효율적이라는 결론에 도달하게 됩니다.

## 새로운 아키텍쳐

### Skia

하지만 이러한 온디맨드-리사이징 아키텍쳐로의 변환에 가장 큰 걸림돌이 있었습니다. 바로 사진의 리사이징에 오랜 시간이 걸린다는 점이었습니다.
비록 아키텍쳐 변화를 통해서 저희가 얻을 수 있는 비용 이득이 크더라도, 비트윈 사용자 경험에 느린 사진 리사이징이 방해가 되어서는 안 되었습니다.

이때 저희가 찾은 것이 바로 [Skia] 라이브러리였습니다. Skia 라이브러리는 Google에 의해서 만들어진 2D 그래픽 라이브러리로써, 크롬이나 안드로이드, 모질라 파이어폭스 등에 사용되고 있었습니다.
그리고 이 라이브러리는 CPU 아키텍쳐에 따라서 인스트럭션 레벨로 매우 잘 최적화가 되어 있었습니다.
저희가 **기존에 쓰고 있던 [ImageMagicK]에 비해서 거의 4배 속도로 이미지 리사이징**을 처리할 수 있었으며, 총 CPU 사용량도 더 적었습니다.
저희는 이 라이브러리를 Python으로 wrapping한 PySkia라는 라이브러리를 내부적으로 만들어서 사진 리사이징에 사용하기로 하였습니다.

### WebP

저희는 여기서 한발 더 나아가 보기로 했습니다. 단순히 리사이징만 Skia로 대체하는 것이 아니라, 원본 사진의 저장도 더 효율적으로 할 방법을 찾게 되었습니다.
그 결과 자연스럽게 떠오른 것이 [비트윈 스티커 시스템]에서 사용되었던 [WebP] 방식이었습니다.
WebP 역시 구글이 만든 이미지 인코딩 방식으로써, **비슷한 화질을 가지는 JPEG에 비해서 약 26% 정도의 용량이 절약**된다는 점에서 장점이 있습니다.

### 온디멘드-리사이징

위에서 언급한 대로 Skia 리사이징과 WebP 원본 저장을 합하여 아래와 같이 필요한 해상도의 사진을 그때그때 리사이징 하는 온디멘드-리사이징 아키텍쳐로 옮겨가게 되었습니다.

![새로운 사진 전송/리사이징 아키텍쳐][newarchitecture]

<figcaption>사용자가 올린 사진은 원본이 WebP로 변환되어 S3에 저장된다. 클라이언트의 요청이 있을 때는 그때그때 요청한 사이즈로 리사이징한 썸네일을 생성해서 내려준다.</figcaption>

리사이저 서버가 사용자의 요청을 받아서 원하는 해상도의 사진을 리사이징해서 내려주기까지 채 100ms가 걸리지 않는데, 이 정도면 사용자의 경험에 영향을 주지 않는다고 판단하였습니다.
리사이저 서버는 업로더 서버와 함께 세계 각지의 AWS Region에 배포되어 있으며, 이는 사용자가 요청한 사진을 최대한 빨리 받아가기 위함입니다.

## 기존 사진 마이그레이션

위와 같은 아키텍쳐 전환을 통해서 새롭게 업로드 되는 사진들은 원본만 WebP로 변환되어 저장한 후 요청이 들어올 때만 온디멘드 리사이징이 되지만, 그동안 비트윈 사용자들이 축적해 놓은 11억 장의 사진은 여전히 여러 사이즈의 썸네일로 미리 리사이징이 되어 있는 비효율적인 상태였습니다.
저희는 이 사진들도 마이그레이션하는 작업에 착수했습니다.

11억 장이나 되는 원본 사진들을 전부 WebP로 변환하고, 나머지 50억 장의 미리 생성된 썸네일 사진을 지우는 작업은 결코 간단한 작업이 아니었습니다.
저희는 이 작업을 AWS의 [Spot Instance]와 [SQS]를 통해서 비용 효율적으로 진행할 수 있었습니다.

### Auto Scaling with Spot instance

마이그레이션 작업은 크게 다섯 단계로 이루어져 있습니다.

- 커플 단위로 작업을 쪼개서 SQS에 쌓아놓습니다.
- Worker가 SQS로부터 단위 작업을 받아와서, 해당 커플에 존재하는 모든 사진을 WebP로 변환하고 S3에 올립니다.
- S3로의 업로드가 확인되면, 그 변경 사항을 DB에 적습니다.
- 기존 썸네일 사진들을 삭제합니다.
- 기존 썸네일이 삭제되었다는 사실을 DB에 적습니다.

작업을 하는 도중에 얼마든지 Worker가 중단되거나 같은 커플에 대한 작업이 두 번 중복되어서 이루어질 위험이 있습니다.
이를 위해서 마이그레이션 작업을 [멱등적]으로 구성하여서 사용자의 사진이 손실되는 등의 사고가 발생하지 않도록 하였습니다.
중간마다 DB에 접근해서 변경된 내용을 기록해야 하는 작업의 특성상, 작업의 병목 구간은 비트윈 DB였습니다.
그리고 사진 인코딩을 바꾸는 작업의 특징상 많은 CPU 자원이 소모될 것으로 생각하였습니다.

DB에 부담이 가지 않는 범위내에서 많은 CPU 자원을 끌어와서 작업을 진행해야 할 필요성이 생긴 것입니다. 이 조건을 만족하게 하기 위해서 SQS를 바라보는 Worker들로 [Auto-scaling] group을 만들었습니다.
그리고 이 Auto-scaling group은 c3.2xlarge와 c3.4xlarge spot instance로 구성되어 있으며, DB의 CPU 사용량을 [메트릭]으로 하여 Scaling이 되도록 하였습니다.
작업은 주로 DB의 부하가 적은 새벽 시간에 집중적으로 이루어졌으며, 이 인코딩 작업은 대략 4일 정도가 소모되었습니다.
작업 과정에서 Tokyo Region에 있던 c4.2xlarge와 c3.4xlarge spot instance를 최대 140대를 사용했고, 총 사용 시간은 6,767시간이었습니다.
사용한 instance의 계산 능력을 ECU로 환산하면 총 303,933 ECU &#183; hour를 작업에 사용하였습니다.
마이그레이션에 사용된 EC2 비용을 바탕으로 계산해 보면, 백만 장의 WebP 인코딩을 위해서 사용한 비용이 $1.8 밖에 되지 않았다는 것을 알 수 있습니다.

작업 과정에서 AWS 서비스에 의외의 병목 구간이 있다는 것을 알게 되었는데, S3 단일 버킷에 1분당 1천만 개 이상의 object에 대한 삭제 요청이 들어오면 Throttling이 걸린다는 사실과 SQS의 in-flight message의 개수가 12만 개를 넘을 수 없다는 것입니다.

## 결과

위의 아키텍쳐 변화와 마이그레이션 작업 후 저희의 S3 비용은 70%가 넘게 감소했으며 전체 인프라 비용의 상당 부분이 감소하였습니다.
온디멘드 리사이징으로의 아키텍쳐 변화는 Storage 비용과 Computation 비용 사이의 교환이라고 볼 수 있는데, 아래 그래프에서 볼 수 있듯이 확연한 비용 절감을 달성할 수 있었습니다.

### 총 마이그레이션 비용

|        항목         |    사용량     | 비용 ($) |
| :-----------------: | :-----------: | :------: |
|  EC2 spot instance  |   6,767 hrs   | 1,959.11 |
|         SQS         |  188,204,104  |  89.59   |
| S3 Put/Get Requests | 2,492,466,860 | 5,608.34 |
|       총비용        |               | 7,657.04 |

### 마이그레이션 결과

|      항목       | Before Migration | After Migration | 감소량 (%) |
| :-------------: | :--------------: | :-------------: | :--------: |
| S3 # of objects |      6.65 B      |     1.17 B      |   82.40    |
|   S3 storage    |      738 TB      |     184 TB      |   75.06    |

### 비용 감소

![S3 저장 및 리사이징 비용의 변화][pricereduction]

<figcaption>사진 저장과 리사이징에 관련된 비용이 68% 감소하였음</figcaption>

## 못다 한 이야기

이번 포스팅에서는 최근에 있었던 비트윈 사진 아키텍쳐의 변화에 대해서 알아보았습니다.
주로 사용자의 경험을 방해하지 않는 조건에서 비용을 아끼는 부분에 중점을 두고 저희 비트윈의 아키텍쳐 변화에 대해서 설명해 드렸습니다.
하지만 이 글에서 미처 언급하지 못한 변화나 개선 사항들에 대해서는 다루지 못했습니다.
Tokyo Region에서 멀리 떨어져 있는 사용자를 위해서 전 세계 여러 Region에 사진 저장/전송 서버를 배포하는 일이나, 사진을 로딩할 때 낮은 해상도로부터 차례대로 로딩되도록 하는 [Progressive JPEG]의 적용, 사진을 아직 받아오지 못했을 때 Placeholder 역할을 할 수 있는 [사진의 대표색]을 찾아내는 방법 등이 그것입니다.
이에 관해서는 후에 자세히 다뤄보도록 하겠습니다.

## 정리

비트윈 개발팀에서는 많은 인프라 비용을 소모하는 기존 썸네일 저장 방식을 개선하여 70%에 가까운 비용 절감 효과를 보았습니다.
기존의 썸네일을 미리 생성해놓는 방식으로부터 클라이언트가 요청할 때 해당 크기의 썸네일을 리사이징해서 내려주는 방식으로 변경하였고, WebP와 Skia등의 새로운 기술을 적용하였습니다.
이를 통해서 사용자 경험에는 거의 영향을 주지 않은 상태로 비용 절감 효과를 볼 수 있었습니다.

[s3]: https://aws.amazon.com/ko/s3/
[fan-out]: https://en.wikipedia.org/wiki/Fan-out
[lru]: https://en.wikipedia.org/wiki/Cache_algorithms#LRU
[skia]: https://skia.org/
[imagemagick]: http://www.imagemagick.org/script/index.php
[비트윈 스티커 시스템]: http://engineering.vcnc.co.kr/2013/06/architecture-of-sticker-system/
[webp]: https://developers.google.com/speed/webp/#how_webp_works_stylefont-weight_bold
[spot instance]: https://aws.amazon.com/ko/ec2/spot/
[sqs]: https://aws.amazon.com/ko/sqs/
[멱등적]: https://ko.wikipedia.org/wiki/%EB%A9%B1%EB%93%B1%EB%B2%95%EC%B9%99
[auto-scaling]: https://aws.amazon.com/ko/autoscaling/
[메트릭]: https://aws.amazon.com/ko/cloudwatch/
[progressive jpeg]: https://optimus.io/support/progressive-jpeg/
[사진의 대표색]: http://lokeshdhakar.com/projects/color-thief/
[s3cachehitrate]: ./s3_cache_hit_rate.png
[oldarchitecture]: ./old_architecture.png
[newarchitecture]: ./new_architecture.png
[pricereduction]: ./price_reduction.png
